{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/imacmattimacmatt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3dc0348c594f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mword2number\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mw2n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycontractions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"condensed.csv\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_tweet_list = []\n",
    "total_tokens = 0  \n",
    "# import the csv file and extract the text entries\n",
    "with open (\"condensed.csv\") as f:\n",
    "    csvReader = csv.DictReader(f)\n",
    "    tweet_list = []\n",
    "    clean_tweet_list = []\n",
    "    time_list = []\n",
    "    Vader_list = []\n",
    "    for row in csvReader:\n",
    "        data = row[\"Tweet_text\"], row[\"Time\"], row[\"Vader_compound\"]\n",
    "        tweet_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweet_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"@ron_fournier: President Donald Trump (just getting used to it)\" Wow, very nice!', '2016-01-20 11:14:50', '0.801')\n"
     ]
    }
   ],
   "source": [
    "print(tweet_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Senator @CoryGardner is doing a terrific job - we just passed his landmark legislation, the Great American Outdoors Act. Strong on our Military, Vets and the Second Amendment, Cory has always had my Complete and Total Endorsement!  ', '2020-08-14 23:41:55', '0.928'), ('...Steve Bullock is totally against your Second Amendment and just received an F rating from the @NRA. Steve, on the other hand, just received an A+ rating. Vote for Steve Daines. He has always had my Complete and Total Endorsement! #MTSEN  ', '2020-08-14 23:43:03', '0.726'), ('Senator @SteveDaines is doing a tremendous job for the people of Montana! He just helped me pass the Great American Outdoors Act, which protects the Montana way of life. His opponent, Steve Bullock, would be an absolute horror for Montana...', '2020-08-14 23:43:03', '0.457'), ('Today, it was my great honor to proudly accept the endorsement of the @NYCPBA! I have deeply and profoundly admired the brave men and women of the #NYPD for my entire life. New York’s Finest are truly the best of the best — I will NEVER let you down! #MAGA  ', '2020-08-15 03:59:18', '0.988')]\n"
     ]
    }
   ],
   "source": [
    "print(tweet_list[-5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweet_list:\n",
    "\ttext = tweet[0]\n",
    "\ttime = tweet[1]\n",
    "\tVader_compound = tweet[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'https.*', '', text)\n",
    "text = text.replace('&amp', '')\n",
    "text = text.replace('U.S.', 'usa')\n",
    "text = text.replace('RT', '')\n",
    "text = text.replace('dems', 'democrats')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()\n",
    "\t\t# make lower case\n",
    "tokens = [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'bring', 'safety', 'to', 'our', 'cities.', 'we', 'will', 'bring', 'hope', 'to', 'our', 'most', 'distressed', 'communities', '—', 'and', 'in', 'everything', 'we', 'do,', 'we', 'will', 'stand', 'in', 'solidarity', 'with', 'the', 'men', 'and', 'women', 'of', 'law', 'enforcement!', 'thank', 'you', 'to', 'the', '@nycpba', 'for', 'your', 'endorsement.', 'god', 'bless', 'the', '#nypd,', 'and', 'god', 'bless', 'america!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = [\"\".join(c for c in word if c.isalnum() or c==\"#\" or c==\"@\") for word in tokens ]\n",
    "\n",
    "tokens = [\"\".join(c for c in word if c.isalnum()) for word in tokens ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'bring', 'safety', 'to', 'our', 'cities', 'we', 'will', 'bring', 'hope', 'to', 'our', 'most', 'distressed', 'communities', '', 'and', 'in', 'everything', 'we', 'do', 'we', 'will', 'stand', 'in', 'solidarity', 'with', 'the', 'men', 'and', 'women', 'of', 'law', 'enforcement', 'thank', 'you', 'to', 'the', 'nycpba', 'for', 'your', 'endorsement', 'god', 'bless', 'the', 'nypd', 'and', 'god', 'bless', 'america']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "content_word_tweet = [w for w in tokens if not w in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring', 'safety', 'cities', 'bring', 'hope', 'distressed', 'communities', '', 'everything', 'stand', 'solidarity', 'men', 'women', 'law', 'enforcement', 'thank', 'nycpba', 'endorsement', 'god', 'bless', 'nypd', 'god', 'bless', 'america']\n"
     ]
    }
   ],
   "source": [
    "print(content_word_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a345152d511c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmytokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"-PRON-\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent_word_tweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmytokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-a345152d511c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmytokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"-PRON-\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent_word_tweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmytokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": [
    "mytokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in content_word_tweet]\n",
    "print(mytokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w)for w in content_word_tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bring safety city bring hope distressed community  everything stand solidarity men woman law enforcement thank nycpba endorsement god bless nypd god bless america\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring', 'safety', 'city', 'bring', 'hope', 'distressed', 'community', ' ', 'everything', 'stand', 'solidarity', 'man', 'woman', 'law', 'enforcement', 'thank', 'nycpba', 'endorsement', 'god', 'bless', 'nypd', 'god', 'bless', 'america']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(lemmatized_output)\n",
    "mytokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc]\n",
    "print(mytokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_string =print(nltk.pos_tag(nltk.word_tokenize(\"Great rally in Iowa! Such wonderful people. Traveling now with @SarahPalinUSA to Tulsa- massive crowd expected!\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pos_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = list(lemmatized_output.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweet_list[-5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['bring'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_wordnet_pos('Great rally in Iowa! Such wonderful people. Traveling now with @SarahPalinUSA to Tulsa- massive crowd expected!')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(lemmatized_output)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tDF = {}\n",
    "\tfor i in range(len(tweets_text)):\n",
    "\t\ttokens = tweets_text[i]\n",
    "\t\tfor w in tokens:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tDF[w].add(i)\n",
    "\t\t\t# .add is a set function (creates a set), so it will only add 1 time, set values have to be unique\n",
    "\t\t\texcept:\n",
    "\t\t\t\tDF[w] = {i}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
